[
  {
    "id": 1,
    "type": "Seleção de Candidatos",
    "context": "Uma empresa de tecnologia implementou um sistema de IA para triagem inicial de currículos para vagas de engenharia de software.",
    "situation": "O sistema foi treinado com currículos de funcionários contratados nos últimos 5 anos. Após 6 meses de uso, a equipe de RH notou que 85% dos currículos aprovados pelo sistema eram de candidatos homens, enquanto apenas 15% eram de mulheres, mesmo com um pool inicial mais equilibrado de candidatos.",
    "bias_type": "Viés de Gênero",
    "bias_explanation": "O sistema aprendeu padrões históricos de contratação que favoreciam candidatos homens, perpetuando a desigualdade de gênero na empresa.",
    "correct_identification": ["Viés de Gênero", "Viés de Representação"],
    "severity": "Alta",
    "solutions": [
      "Balancear os dados de treinamento com representação equitativa",
      "Implementar métricas de equidade no processo de avaliação",
      "Auditorias regulares dos resultados por grupos demográficos",
      "Envolver equipes diversas no desenvolvimento do sistema"
    ]
  },
  {
    "id": 2,
    "type": "Reconhecimento Facial",
    "context": "Uma escola implementou um sistema de reconhecimento facial para controle de acesso e monitoramento de presença dos alunos.",
    "situation": "Após algumas semanas, professores relataram que o sistema frequentemente falhava em reconhecer alunos com tons de pele mais escuros, resultando em atrasos no acesso às salas e registros incorretos de presença. Alguns alunos começaram a se sentir constrangidos e discriminados.",
    "bias_type": "Viés Racial",
    "bias_explanation": "Sistemas de reconhecimento facial frequentemente apresentam menor acurácia para pessoas com tons de pele mais escuros devido a dados de treinamento não representativos.",
    "correct_identification": ["Viés Racial", "Viés de Representação"],
    "severity": "Alta",
    "solutions": [
      "Usar datasets de treinamento mais diversos e representativos",
      "Testar o sistema com diferentes grupos demográficos antes da implementação",
      "Implementar mecanismos alternativos de acesso",
      "Monitorar continuamente a performance por grupos demográficos"
    ]
  },
  {
    "id": 3,
    "type": "Recomendação de Conteúdo",
    "context": "Uma plataforma educacional online usa IA para recomendar cursos e materiais de estudo personalizados para cada aluno.",
    "situation": "Análises mostraram que o sistema consistentemente recomenda cursos de ciências exatas (matemática, física, programação) mais frequentemente para alunos homens, enquanto sugere cursos de humanidades e ciências sociais mais para alunas. Isso ocorre mesmo quando os históricos acadêmicos e interesses declarados são similares.",
    "bias_type": "Viés de Gênero",
    "bias_explanation": "O algoritmo aprendeu estereótipos sociais sobre preferências acadêmicas baseadas em gênero, limitando as oportunidades de exploração para todos os alunos.",
    "correct_identification": ["Viés de Gênero", "Viés Cultural"],
    "severity": "Média",
    "solutions": [
      "Remover ou minimizar o uso de gênero como feature",
      "Implementar diversidade nas recomendações",
      "Permitir que usuários ajustem suas preferências explicitamente",
      "Testar regularmente para padrões discriminatórios"
    ]
  },
  {
    "id": 4,
    "type": "Avaliação Automática",
    "context": "Um sistema de IA foi desenvolvido para avaliar automaticamente redações de alunos em exames padronizados.",
    "situation": "Professores notaram que redações que mencionavam experiências culturais específicas de minorias étnicas ou contextos socioeconômicos desfavorecidos recebiam pontuações sistematicamente mais baixas, mesmo quando bem estruturadas e argumentadas. O sistema parecia valorizar referências culturais mais mainstream.",
    "bias_type": "Viés Cultural",
    "bias_explanation": "O sistema foi treinado com redações que refletiam principalmente experiências de grupos majoritários, não reconhecendo adequadamente a diversidade cultural nas expressões dos alunos.",
    "correct_identification": ["Viés Cultural", "Viés Socioeconômico"],
    "severity": "Alta",
    "solutions": [
      "Incluir avaliadores humanos diversos no processo de treinamento",
      "Expandir o conjunto de dados com redações de backgrounds diversos",
      "Focar na qualidade da argumentação independente do conteúdo cultural",
      "Implementar revisão humana para casos borderline"
    ]
  },
  {
    "id": 5,
    "type": "Tradução Automática",
    "context": "Uma escola internacional usa um sistema de tradução automática para comunicação entre alunos, pais e professores de diferentes nacionalidades.",
    "situation": "Foi observado que quando profissões eram mencionadas em idiomas com gênero neutro, o sistema consistentemente traduzia 'médico' e 'engenheiro' para o masculino, enquanto 'enfermeiro' e 'professor' eram traduzidos para o feminino, independentemente do contexto ou da pessoa referenciada.",
    "bias_type": "Viés de Gênero",
    "bias_explanation": "O sistema de tradução incorporou estereótipos de gênero presentes nos dados de treinamento, associando certas profissões a gêneros específicos.",
    "correct_identification": ["Viés de Gênero", "Viés Cultural"],
    "severity": "Média",
    "solutions": [
      "Usar linguagem neutra sempre que possível",
      "Implementar detecção de contexto para determinar gênero apropriado",
      "Treinar com dados mais equilibrados em termos de gênero",
      "Oferecer opções de tradução alternativas"
    ]
  }
]
